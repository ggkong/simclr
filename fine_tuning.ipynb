{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:52:29.174498100Z",
     "start_time": "2025-03-25T03:52:18.981988Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 构建分类数据集\n",
    "class APTOSClassifyDataset(Dataset):\n",
    "    def __init__(self, image_dir, csv_file, image_size=224, mode='train'):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.mode = mode\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.HorizontalFlip(p=0.5 if mode == 'train' else 0.0),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                        std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['id_code'] + '.png')\n",
    "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        image = self.transform(image=image)['image']\n",
    "        label = row['diagnosis']\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:52:29.189002900Z",
     "start_time": "2025-03-25T03:52:29.179986500Z"
    }
   },
   "id": "d9371fa6397438d0"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class EfficientNetClassifier(nn.Module):\n",
    "    def __init__(self, backbone='efficientnet_b0', num_classes=5, pretrained_encoder_path=None):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(backbone, pretrained=False, num_classes=0)\n",
    "        \n",
    "        if pretrained_encoder_path:\n",
    "            state_dict = torch.load(pretrained_encoder_path, map_location='cpu')\n",
    "            self.encoder.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"✅ Loaded SimCLR encoder from: {pretrained_encoder_path}\")\n",
    "\n",
    "        self.classifier = nn.Linear(self.encoder.num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:52:29.208650600Z",
     "start_time": "2025-03-25T03:52:29.193500700Z"
    }
   },
   "id": "132e66c3dc81ae20"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:52:29.222517800Z",
     "start_time": "2025-03-25T03:52:29.205661500Z"
    }
   },
   "id": "c002d981b0375752"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_dataset = APTOSClassifyDataset('fine_tune/train_split', 'fine_tune/train.csv', mode='train')\n",
    "test_dataset = APTOSClassifyDataset('fine_tune/test_split', 'fine_tune/test.csv', mode='test')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:52:32.471015500Z",
     "start_time": "2025-03-25T03:52:32.452364300Z"
    }
   },
   "id": "fa896f32a07550a6"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:52:33.163660300Z",
     "start_time": "2025-03-25T03:52:33.142227400Z"
    }
   },
   "id": "d485a98ef9566417"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded SimCLR encoder from: checkpoints/simclr_epoch_9.pth\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNetClassifier(pretrained_encoder_path='checkpoints/simclr_epoch_9.pth').to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:52:35.631864600Z",
     "start_time": "2025-03-25T03:52:35.424595Z"
    }
   },
   "id": "b45ffb65e8b48c6b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:52:40.414398500Z",
     "start_time": "2025-03-25T03:52:40.404437900Z"
    }
   },
   "id": "eb68309fb5fe9d4b"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 1.5337 - Val Acc: 0.3000 - Kappa: 0.0000\n",
      "Epoch 2/20 - Train Loss: 1.4284 - Val Acc: 0.3000 - Kappa: 0.0000\n",
      "Epoch 3/20 - Train Loss: 1.3271 - Val Acc: 0.3000 - Kappa: 0.0000\n",
      "Epoch 4/20 - Train Loss: 1.1888 - Val Acc: 0.3000 - Kappa: 0.0000\n",
      "Epoch 5/20 - Train Loss: 1.0532 - Val Acc: 0.3000 - Kappa: 0.0000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     12\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 14\u001B[0m     train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m     17\u001B[0m all_preds, all_labels \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 20\n",
    "# for epoch in range(1, num_epochs + 1):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         logits = model(images)\n",
    "#         loss = criterion(logits, labels)\n",
    "# \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "# \n",
    "#         train_loss += loss.item()\n",
    "# \n",
    "#     model.eval()\n",
    "#     all_preds, all_labels = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images = images.to(device)\n",
    "#             outputs = model(images)\n",
    "#             preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "#             all_preds.extend(preds)\n",
    "#             all_labels.extend(labels.numpy())\n",
    "# \n",
    "#     acc = accuracy_score(all_labels, all_preds)\n",
    "#     kappa = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "#     print(f\"Epoch {epoch}/{num_epochs} - Train Loss: {train_loss/len(train_loader):.4f} - Val Acc: {acc:.4f} - Kappa: {kappa:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:47:38.005618100Z",
     "start_time": "2025-03-25T03:42:36.872886900Z"
    }
   },
   "id": "6ef09a741fb7e211"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnosis\n",
      "0    346\n",
      "2    186\n",
      "1     71\n",
      "4     58\n",
      "3     38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('fine_tune/train.csv')\n",
    "print(df['diagnosis'].value_counts())\n",
    "# 样本不平衡"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:52:52.269469100Z",
     "start_time": "2025-03-25T03:52:52.257190500Z"
    }
   },
   "id": "679022d054e42a77"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('fine_tune/train.csv')\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(train_df['diagnosis']),\n",
    "                                     y=train_df['diagnosis'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:52:53.472271500Z",
     "start_time": "2025-03-25T03:52:53.456130400Z"
    }
   },
   "id": "6d0688095704f9f4"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 1.5545 - Train Acc: 0.3763 - Val Acc: 0.3000\n",
      "Epoch 2/20 - Train Loss: 1.4562 - Train Acc: 0.4764 - Val Acc: 0.3000\n",
      "Epoch 3/20 - Train Loss: 1.3632 - Train Acc: 0.5622 - Val Acc: 0.3000\n",
      "Epoch 4/20 - Train Loss: 1.2550 - Train Acc: 0.5880 - Val Acc: 0.3000\n",
      "Epoch 5/20 - Train Loss: 1.1213 - Train Acc: 0.6252 - Val Acc: 0.3000\n",
      "Epoch 6/20 - Train Loss: 1.0129 - Train Acc: 0.6552 - Val Acc: 0.3300\n",
      "Epoch 7/20 - Train Loss: 0.8905 - Train Acc: 0.6924 - Val Acc: 0.5767\n",
      "Epoch 8/20 - Train Loss: 0.7871 - Train Acc: 0.7096 - Val Acc: 0.6933\n",
      "Epoch 9/20 - Train Loss: 0.7322 - Train Acc: 0.7182 - Val Acc: 0.6933\n",
      "Epoch 10/20 - Train Loss: 0.7150 - Train Acc: 0.7239 - Val Acc: 0.7100\n",
      "Epoch 11/20 - Train Loss: 0.6651 - Train Acc: 0.7396 - Val Acc: 0.7000\n",
      "Epoch 12/20 - Train Loss: 0.6277 - Train Acc: 0.7425 - Val Acc: 0.7000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 27\u001B[0m\n\u001B[0;32m     25\u001B[0m all_preds, all_labels \u001B[38;5;241m=\u001B[39m [], []\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 27\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m test_loader:\n\u001B[0;32m     28\u001B[0m         images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     29\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m model(images)\n",
      "File \u001B[1;32mD:\\soft\\miniconda\\envs\\torch200\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\soft\\miniconda\\envs\\torch200\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    676\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    677\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 678\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    680\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\soft\\miniconda\\envs\\torch200\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\soft\\miniconda\\envs\\torch200\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[2], line 21\u001B[0m, in \u001B[0;36mAPTOSClassifyDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     19\u001B[0m row \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf\u001B[38;5;241m.\u001B[39miloc[idx]\n\u001B[0;32m     20\u001B[0m img_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_dir, row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid_code\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.png\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 21\u001B[0m image \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(\u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_path\u001B[49m\u001B[43m)\u001B[49m, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2RGB)\n\u001B[0;32m     22\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(image\u001B[38;5;241m=\u001B[39mimage)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     23\u001B[0m label \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiagnosis\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds, train_labels = [], []\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "        train_preds.extend(preds)\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "\n",
    "    # === 验证 ===\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    val_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs} \"\n",
    "          f\"- Train Loss: {train_loss / len(train_loader):.4f} \"\n",
    "          f\"- Train Acc: {train_acc:.4f} \"\n",
    "          f\"- Val Acc: {val_acc:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T04:04:14.185504400Z",
     "start_time": "2025-03-25T03:53:06.208145500Z"
    }
   },
   "id": "554bb994f2be78ef"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 微调后的模型已保存。\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('finetune_checkpoints', exist_ok=True)\n",
    "torch.save(model.state_dict(), 'finetune_checkpoints/finetuned_model.pth')\n",
    "print(\"✅ 微调后的模型已保存。\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T04:04:23.863977200Z",
     "start_time": "2025-03-25T04:04:23.807151900Z"
    }
   },
   "id": "97b2f4f33a0d358"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0,1,2,3,4], yticklabels=[0,1,2,3,4])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix (Val Set)')\n",
    "plt.savefig('finetune_checkpoints/confusion_matrix.png')\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T04:12:34.592428200Z",
     "start_time": "2025-03-25T04:12:34.461642700Z"
    }
   },
   "id": "8c426d9e9e304393"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = []  # shape: N x 128\n",
    "labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, lbls in test_loader:\n",
    "        images = images.to(device)\n",
    "        feats = model.encoder(images).cpu().numpy()\n",
    "        features.extend(feats)\n",
    "        labels.extend(lbls.numpy())\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "reduced = tsne.fit_transform(features)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(reduced[:, 0], reduced[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Class\")\n",
    "plt.title(\"t-SNE Visualization of Features (Val Set)\")\n",
    "plt.savefig('finetune_checkpoints/tsne_features.png')\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T04:13:14.224363800Z",
     "start_time": "2025-03-25T04:12:58.026037100Z"
    }
   },
   "id": "c498f3e83ad55647"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\soft\\miniconda\\envs\\torch200\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\soft\\miniconda\\envs\\torch200\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\soft\\miniconda\\envs\\torch200\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# classification_report\n",
    "report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose().iloc[:5]  # 前5类（0-4）\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_report[['precision', 'recall', 'f1-score']].plot(kind='bar')\n",
    "plt.title(\"Classification Report per Class\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, axis='y')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('finetune_checkpoints/classification_report.png')\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T05:15:58.001496300Z",
     "start_time": "2025-03-25T05:15:57.843583500Z"
    }
   },
   "id": "9ea75d06b5ea87"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "classes = np.unique(all_labels)\n",
    "pred_counts = [np.sum(np.array(all_preds) == c) for c in classes]\n",
    "true_counts = [np.sum(np.array(all_labels) == c) for c in classes]\n",
    "\n",
    "x = np.arange(len(classes))\n",
    "plt.bar(x - 0.2, true_counts, width=0.4, label='True')\n",
    "plt.bar(x + 0.2, pred_counts, width=0.4, label='Predicted')\n",
    "plt.xticks(x, classes)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class Distribution: True vs Predicted\")\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.savefig('finetune_checkpoints/class_distribution.png')\n",
    "plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T05:16:15.642074300Z",
     "start_time": "2025-03-25T05:16:15.558593200Z"
    }
   },
   "id": "7ff00f3cf7df75bb"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, lbls in test_loader:\n",
    "        images = images.to(device)\n",
    "        feats = model.encoder(images).cpu().numpy()\n",
    "        features.extend(feats)\n",
    "        labels.extend(lbls.numpy())\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "reduced = tsne.fit_transform(features)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(reduced[:, 0], reduced[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Class\")\n",
    "plt.title(\"t-SNE Visualization of Val Set Features\")\n",
    "plt.savefig('finetune_checkpoints/tsne_visualization.png')\n",
    "plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T05:16:42.953681700Z",
     "start_time": "2025-03-25T05:16:25.025459400Z"
    }
   },
   "id": "3f6811775e57a43c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bba8e4303dc3f93b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
